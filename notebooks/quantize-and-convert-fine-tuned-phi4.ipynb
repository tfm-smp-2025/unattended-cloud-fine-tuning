{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3406b7d",
      "metadata": {
        "id": "a3406b7d"
      },
      "source": [
        "# Model conversion to GGUF notebook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start time\n",
        "!date"
      ],
      "metadata": {
        "id": "1sQhHMM3hH56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24322afb-b945-46d8-b298-9f7a8e9d319c"
      },
      "id": "1sQhHMM3hH56",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May 16 01:54:17 PM UTC 2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = \"r16_a16_s0_d0_bnone_l0.0001_4bFalse_20250514_2224\"\n",
        "checkpoint = \"4745\""
      ],
      "metadata": {
        "id": "alEj6AtEVZpq"
      },
      "id": "alEj6AtEVZpq",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "_xDYWzO-Yayb"
      },
      "id": "_xDYWzO-Yayb"
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "def get_secret(prompt, secret_name, secret_input=True):\n",
        "  try:\n",
        "    from google.colab import userdata\n",
        "    result = userdata.get(secret_name)\n",
        "    assert result is not None\n",
        "  except:\n",
        "    if secret_input:\n",
        "      result = getpass(prompt)\n",
        "    else:\n",
        "      result = input(prompt)\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "oXM0JQXeWrlv"
      },
      "id": "oXM0JQXeWrlv",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "    !pip install -U transformers\n",
        "\n",
        "from unsloth import FastLanguageModel  # Load unsloth ASAP"
      ],
      "metadata": {
        "id": "YAkKPixLWuZA"
      },
      "id": "YAkKPixLWuZA",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download pre-trained checkpoint"
      ],
      "metadata": {
        "id": "3oyvCKsOXCbq"
      },
      "id": "3oyvCKsOXCbq"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if (\n",
        "    (not os.path.exists(os.path.expanduser(\"~/.ssh/id_rsa\")))\n",
        "    or (not os.path.exists(os.path.expanduser(\"~/.ssh/known_hosts\")))\n",
        "):\n",
        "  SSHKEY = get_secret('Result pusher SSH key: ', 'TFM_SSH_PUSHER_KEY')\n",
        "\n",
        "  !mkdir ~/.ssh\n",
        "\n",
        "  # Read locally with `cat ~/.ssh/result-pusher|tr '\\n' '$';echo`\n",
        "  with open(os.path.expanduser(\"~/.ssh/id_rsa\"), 'wt') as f:\n",
        "    f.write(SSHKEY.replace('$', '\\n'))\n",
        "\n",
        "  !chmod 0600 ~/.ssh/id_rsa\n",
        "  !ssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub\n",
        "  !chmod 0600 ~/.ssh/id_rsa.pub\n",
        "\n",
        "  # This won't copy the client key (not needed), but it will initialize the server's on the client\n",
        "  !ssh-copy-id -i ~/.ssh/id_rsa -o StrictHostKeyChecking=accept-new result-pusher@kb.tfm.codigoparallevar.com\n",
        "\n",
        "  del SSHKEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owXoATDXXJD5",
        "outputId": "884aa3f2-6597-458a-e9df-e6864bce689e"
      },
      "id": "owXoATDXXJD5",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/root/.ssh/id_rsa.pub\"\n",
            "/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n",
            "\n",
            "/usr/bin/ssh-copy-id: WARNING: All keys were skipped because they already exist on the remote system.\n",
            "\t\t(if you think this is a mistake, you may want to use -f option)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rsync -HPrz --mkpath \\\n",
        "  result-pusher@kb.tfm.codigoparallevar.com:fine-tuning/fine-tuned/\"$trainset\"/checkpoint-\"$checkpoint\"/ \\\n",
        "    fine-tune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEbcZ6xaXEmk",
        "outputId": "f4da8e9b-354b-451a-fe95-b658863738ca"
      },
      "id": "vEbcZ6xaXEmk",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "receiving incremental file list\n",
            "created 1 directory for fine-tune\n",
            "README.md\n",
            "          5,087 100%    4.85MB/s    0:00:00 (xfr#1, to-chk=12/14)\n",
            "adapter_config.json\n",
            "            843 100%  823.24kB/s    0:00:00 (xfr#2, to-chk=11/14)\n",
            "adapter_model.safetensors\n",
            "    262,219,392 100%   15.91MB/s    0:00:15 (xfr#3, to-chk=10/14)\n",
            "merges.txt\n",
            "        916,646 100%    1.57MB/s    0:00:00 (xfr#4, to-chk=9/14)\n",
            "optimizer.pt\n",
            "    133,785,108 100%   15.70MB/s    0:00:08 (xfr#5, to-chk=8/14)\n",
            "rng_state.pth\n",
            "         14,244 100%  434.69kB/s    0:00:00 (xfr#6, to-chk=7/14)\n",
            "scheduler.pt\n",
            "          1,064 100%   32.47kB/s    0:00:00 (xfr#7, to-chk=6/14)\n",
            "special_tokens_map.json\n",
            "            456 100%   13.92kB/s    0:00:00 (xfr#8, to-chk=5/14)\n",
            "tokenizer.json\n",
            "      7,153,264 100%   48.04MB/s    0:00:00 (xfr#9, to-chk=4/14)\n",
            "tokenizer_config.json\n",
            "         17,987 100%  122.84kB/s    0:00:00 (xfr#10, to-chk=3/14)\n",
            "trainer_state.json\n",
            "         43,838 100%  295.25kB/s    0:00:00 (xfr#11, to-chk=2/14)\n",
            "training_args.bin\n",
            "          5,304 100%   35.72kB/s    0:00:00 (xfr#12, to-chk=1/14)\n",
            "vocab.json\n",
            "      1,612,637 100%    8.79MB/s    0:00:00 (xfr#13, to-chk=0/14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load pre-trained model"
      ],
      "metadata": {
        "id": "T6lQKiO2XAyX"
      },
      "id": "T6lQKiO2XAyX"
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Phi-4\",\n",
        "    max_seq_length = 16384, # max_seq_length,\n",
        "    load_in_4bit = False, # load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(model, \"fine-tune\",torch_dtype=torch.float16,is_trainable=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569,
          "referenced_widgets": [
            "454729cacebc4a1eab3df834ac6c471d",
            "f29ce180dfce4e3787bba8f0f3cb1743",
            "dbcca8d4362d4eb0bc3f731996630b4a",
            "b5ef222392c54c00a3aa96f130d42297",
            "6a4dcaa899c64574a18a086dad0aa15b",
            "506a580e0da645e8a2ae7cf350eba80b",
            "cd272ba4df6e400e9611bf04c4472727",
            "2cff2c842cc448ebbff4198c26ef36c9",
            "963fa7ca3faa40449273b686f6093234",
            "a466cfa87ce64a919937e3e5d8e93bf0",
            "fb98d399e1d945d6bc9e40c6b4200955",
            "f516bac9163144fdbaa4f494d19f70b9",
            "0417b03a471446638b182fd1d0869c5c",
            "8b80d3c0dba24efca01eb8bf8a1f8303",
            "9ec2f682e40a4f93a0ff011f0fe76266",
            "0e305a0dcf5f462f91277137983aa13f",
            "a4d0701f3ba549aca169f5b126254fde",
            "8287adc7ff964bb1a0b563e4aa4ace41",
            "32b1f9aae33c4a4c8e53e25b66999391",
            "2546fad4e90746edb0e7310382c7cd9a",
            "83ed389e5d3f46d8bcf3167e80b0df6b",
            "9842b56467194eedb82e5c5198c83dbd",
            "13411f05008d4c918242dc86fc11ab22",
            "3966ebb0bb6c4af3a4c646288c5ad6db",
            "e467f4f285b2405a8a17095c677e69dc",
            "893a173f520d49e8810533106e6b5ba2",
            "da50b878c8aa4f0aaacb85ea1639877a",
            "cc341fb160dd4cc9bf7dc0dcac7cd62b",
            "a2d174540733429f90c141fcd20b45db",
            "6edef4f5d95b41469d7a839b30351957",
            "0ad0f9448a14463cb852c76492bef0f9",
            "f85be3d35c4b46bf9f064564bb7b7b83",
            "08ae7efe8a794d75aa8d3555cd734191",
            "af005114bef944b5bc049bbfcb3d9157",
            "3e33bba6d10140d0a4e6a4c4953900e1",
            "de958341322f471081fb7437beff50bd",
            "f100cd00502a4cce93a1f84bbc9f3b68",
            "3121821ce7194a24a73438204b0201a3",
            "8d39f534a13140b3b46ce980f92a5847",
            "8a70f90a68874eec99718f734d920455",
            "944bfb6cf29f4df68788d498751f53de",
            "a5000214e5024d69ab35e56dda6fc53b",
            "3d3f1e578ee242f0930488b8f3925c29",
            "a40d61ddb5d7441f98827ea4c0a6552a",
            "bb64c3bf5bd14991b5a994fb5ae7c887",
            "4e138b85ebe747c196e1355a750a903c",
            "e7471a7b5e644c2491552c000b2a4f85",
            "be5170065cf7463bb3e4312a81d7b50b",
            "b295a7fa3f534dcdb6c49202bf211b63",
            "019ddfc48f7f4cf39cb099eab1c25617",
            "437f57ee7c83463cad2f0b0e895090f8",
            "12b923f5fe1f44a8892aa12ac4f913d6",
            "51f3aa7abeda496296c7f957edfc078c",
            "3e559cc5e920472281442a08d3e2cb22",
            "abd26025413b4ac6ac2887d011186b70",
            "22ba7c98218b4c4985f60f30f0467aeb",
            "0ec48f9da76b4dec8acaefd12253ae14",
            "1f054aa82301497499dcbb835722712a",
            "a540162b1353497a9539599b06536daa",
            "6a7dc926dee1426fa9bf1a7cf038aadf",
            "6855aef917824d3c86a4e2a3c25bbbab",
            "56415b8e01a847538429f73d9db09ba5",
            "25e8ee6cfc17486186936a9e0755cc58",
            "cffad697d3bc4a40b6799ee49d4f24f6",
            "a3f2da1690594306abba446c43054cc1",
            "9e6450c597ec479a8fdb352e4d6a801e",
            "3325fc9f7d0d4d09b861b45fb5ecc52e",
            "0d00a7bc34fc44d596f61e3617d8902c",
            "de1e9946bdb74737a789d8115cd298f1",
            "c5db8d88f42d461dab6659f10a6d4d44",
            "e8f57f9c5ae64ad6a1e6ac10ae998bfd",
            "5eaf212cba7b4a178f1e5b9dab223e51",
            "443882bed12946b9a2e6c3eb0ba16f4d",
            "23852c63fcfa4764978110f849b26c7f",
            "881dc453920f4b6690d050b64d86a6dc",
            "65bfa219ef624f40a8183a202754cd5e",
            "fd915b67e1e243ab8ba045b9d3672612",
            "db82e45803504f3e92264bef4ea2a6ab",
            "93460b02c2f5449fa02bc02db39d2eed",
            "424c41cf46f14db5b9173f9fa5a8d065",
            "a8b8cbaa2dca4d57bb2eb0a0afe74fdc",
            "456a1c976dfe4e91abf3093ac4bf511a",
            "59bde084d81145dba069ee0fcd45caff",
            "b092c00f75bb4f84afd3db1a931cacdd",
            "4421d7115c1c423f839c99b24162fec8",
            "3b2b73e7d13649639e27c4a908aed993",
            "84db028fc23f4dec84c291d2720232f8",
            "fc82395c2d594ddfa0cfeb377480e93e",
            "e2a3f27543894789abef2418ceadd72c",
            "2c2a6ab52a7941b28e0f9a9f127ce3e4",
            "74fb43a3edce4ef3b494cb7f4782b547",
            "1ea1abf3dfde4424bcbe6d025637868f",
            "1c33e3b569f24106b84aee21b545b11e",
            "05af7d6a236a442880e4bcf0c2343f4f",
            "395ec87e10fb43a29fa2a94826900997",
            "bcca808b3d9a46b6b3b25d940fc093b9",
            "b5da200f4d2f4d3eb810cfc19b0a4124",
            "f13cb5783f074090866add3b3c5e924d",
            "e3b443aacc654b93a7ac76c92a240208",
            "d6fb2f4bc56644a4b302b8e878d6ea76",
            "1e77d80df41444ee9c2aa41b9a67c125",
            "669114be8e434d5790a1e94fa4db42b2",
            "e06a6237698f4840bbbd6d2ca14425a8",
            "6c3451649bbb48aeaec05fea1a486fad",
            "6b19232948e4415290d65e6a3f5061de",
            "95f7e5c8603c4835939ac3278150b75c",
            "8d2d25bc90364d0dabfe990871b9992f",
            "74f916ad3f41454aa417c6bbdc4d4949",
            "31dacffb94da477fa1d769e2f3486ca5",
            "1d0972522746445981633d6f1f41346a",
            "3a36f0f0cda04472832ac47a06a41c57",
            "0af851b729c04386a3c18d6011ab498d",
            "6544f59a839f4e8e99037c960bc2ab08",
            "eb1aed097cdf455597d6bf9552c4146b",
            "0fb8fff3759e48ca97b0bc35a9d474a4",
            "ec50996065ea429db82cdd84d1218862",
            "0006424cf72c4f2abfb6ecf05ecbc812",
            "d4913d5ffea04a8f942bf8a046ff94c3",
            "80d0b6522ce64db2a8d41bcaedc0ae83",
            "73c19736c4194fb18f230ee649b840d7",
            "3b2d5328c42a40c0b44ab99c0030465f",
            "7dc2833fbe554bbda81ed611e52d326a",
            "dce86e3a461942389e5a93be5ab59989",
            "145d32b5fc554ee7b0f130241914add2",
            "0b4c9224ee1849008dabd303948fc333",
            "1bda6f402e914589824eaf4d626188df",
            "2ec55b14d2c14139981b8099df635939",
            "eb508be9310a476087946536c4860b43",
            "06cf576551f543368f32496ecd971389",
            "0e06fea0936a443ea8f8d622dec0b43a",
            "89016455188c45929d192681f2225cd6",
            "fe90be9a01444b36bef06e092f29d71d",
            "55d82245998e4ae79c2038fd6ada8a43",
            "71667b5c8fb74452acf930de576d1831",
            "9047e24253f148ac8efa801707be781a",
            "1288bc80818b4317b389c19bde8c41db",
            "c3aeb290af764f208362c5f2bfdcb523",
            "5a804ccd0f074e71b2a040b0f1227645",
            "65e3450435be4e94b330014b0e7b9eb1",
            "9b96aed19ffb43b0855f773b772c24e4",
            "6dddfeb5fa3f46c8befd97b761d60a09",
            "c00179a78a9146d18b2d9d411d739133",
            "69a84d9652dd41af9d9799c3f5f5e232",
            "8d66c4c00d674d49ad2bb4d2391e31c9",
            "28a7574fe5e2471e872eb0802a7e4dc9",
            "96bc9c9d5cdd47edbc943ae23a6a5f94",
            "184ff96c7d914278b9f2489d67eed602",
            "bc12e3e580ad4e3fb2325f19ec1dfe50",
            "4e82b3caec784dd0b9aa1d63b5e6f107",
            "eb7e47672e5a4884bd96e899f4e5890b",
            "62ceaab9d7064ac8b7680c9f83928cfe",
            "11b2b3ef92c84812855fd67ea09820ad",
            "8b03bb1ad4ef403796c489233611324a",
            "363d723333bd47f3a5b504a52736308c"
          ]
        },
        "id": "w8D_JMuYXE-Q",
        "outputId": "1fcda1ea-a72b-499c-83ec-af0c915a8237"
      },
      "id": "w8D_JMuYXE-Q",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.5.4: Fast Llama patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/29.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "454729cacebc4a1eab3df834ac6c471d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00006.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f516bac9163144fdbaa4f494d19f70b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00006.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13411f05008d4c918242dc86fc11ab22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00006.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af005114bef944b5bc049bbfcb3d9157"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00006.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb64c3bf5bd14991b5a994fb5ae7c887"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00005-of-00006.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22ba7c98218b4c4985f60f30f0467aeb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00006-of-00006.safetensors:   0%|          | 0.00/4.62G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3325fc9f7d0d4d09b861b45fb5ecc52e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db82e45803504f3e92264bef4ea2a6ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2a3f27543894789abef2418ceadd72c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/18.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6fb2f4bc56644a4b302b8e878d6ea76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.61M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a36f0f0cda04472832ac47a06a41c57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/917k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dc2833fbe554bbda81ed611e52d326a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/456 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55d82245998e4ae79c2038fd6ada8a43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.15M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d66c4c00d674d49ad2bb4d2391e31c9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save to GGUF"
      ],
      "metadata": {
        "id": "XTLWpEVmXr_l"
      },
      "id": "XTLWpEVmXr_l"
    },
    {
      "cell_type": "code",
      "source": [
        "outname = f\"phi-4-{trainset}-cp-{checkpoint}\""
      ],
      "metadata": {
        "id": "WJ36yQ1wUgri"
      },
      "id": "WJ36yQ1wUgri",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fix llama.cpp\n",
        "\n",
        "- See: https://github.com/unslothai/unsloth/issues/748#issuecomment-2238395604"
      ],
      "metadata": {
        "id": "3VdE-nqcxr5n"
      },
      "id": "3VdE-nqcxr5n"
    },
    {
      "cell_type": "code",
      "source": [
        "!bash -c 'git clone --depth=1 --single-branch -b b3345 https://github.com/ggml-org/llama.cpp'\n",
        "!bash -c 'cd llama.cpp && git submodule update --init --recursive'\n",
        "!bash -c 'cd llama.cpp && make clean'\n",
        "!bash -c 'cd llama.cpp && make all -j'"
      ],
      "metadata": {
        "id": "r30rZoQnx0_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ccfec50-aef6-43ea-94ec-1498d833f254"
      },
      "id": "r30rZoQnx0_P",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 1020, done.\u001b[K\n",
            "remote: Counting objects: 100% (1020/1020), done.\u001b[K\n",
            "remote: Compressing objects: 100% (764/764), done.\u001b[K\n",
            "remote: Total 1020 (delta 233), reused 734 (delta 209), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (1020/1020), 18.86 MiB | 19.81 MiB/s, done.\n",
            "Resolving deltas: 100% (233/233), done.\n",
            "Note: switching to '2ee44c9a1865a928ccbbc16a2d7841d7513f31c1'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "Submodule 'kompute' (https://github.com/nomic-ai/kompute.git) registered for path 'ggml/src/kompute'\n",
            "Cloning into '/content/llama.cpp/ggml/src/kompute'...\n",
            "Submodule path 'ggml/src/kompute': checked out '4565194ed7c32d1d2efa32ceab4d3c6cae006306'\n",
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE \n",
            "I NVCCFLAGS: -std=c++11 -O3 \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "rm -vrf *.dot libllava.a llama-baby-llama llama-batched llama-batched-bench llama-bench llama-benchmark-matmult llama-cli llama-convert-llama2c-to-ggml llama-embedding llama-eval-callback llama-export-lora llama-finetune llama-gbnf-validator llama-gguf llama-gguf-hash llama-gguf-split llama-gritlm llama-imatrix llama-infill llama-llava-cli llama-lookahead llama-lookup llama-lookup-create llama-lookup-merge llama-lookup-stats llama-parallel llama-passkey llama-perplexity llama-q8dot llama-quantize llama-quantize-stats llama-retrieval llama-save-load-state llama-server llama-simple llama-speculative llama-tokenize llama-train-text-from-scratch llama-vdot llama-cvector-generator tests/test-c.o tests/test-autorelease tests/test-backend-ops tests/test-chat-template tests/test-double-float tests/test-grad0 tests/test-grammar-integration tests/test-grammar-parser tests/test-json-schema-to-grammar tests/test-llama-grammar tests/test-model-load-cancel tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-rope tests/test-sampling tests/test-tokenizer-0 tests/test-tokenizer-1-bpe tests/test-tokenizer-1-spm\n",
            "rm -rvf src/*.o\n",
            "rm -rvf tests/*.o\n",
            "rm -rvf examples/*.o\n",
            "rm -rvf common/*.o\n",
            "rm -rvf *.a\n",
            "rm -rvf *.dll\n",
            "rm -rvf *.so\n",
            "rm -rvf *.dot\n",
            "rm -rvf ggml/*.a\n",
            "rm -rvf ggml/*.dll\n",
            "rm -rvf ggml/*.so\n",
            "rm -vrf ggml/src/*.o\n",
            "rm -rvf common/build-info.cpp\n",
            "rm -vrf ggml/src/ggml-metal-embed.metal\n",
            "rm -vrf ggml/src/ggml-cuda/*.o\n",
            "rm -vrf ggml/src/ggml-cuda/template-instances/*.o\n",
            "rm -rvf libllava.a llama-baby-llama llama-batched llama-batched-bench llama-bench llama-benchmark-matmult llama-cli llama-convert-llama2c-to-ggml llama-embedding llama-eval-callback llama-export-lora llama-finetune llama-gbnf-validator llama-gguf llama-gguf-hash llama-gguf-split llama-gritlm llama-imatrix llama-infill llama-llava-cli llama-lookahead llama-lookup llama-lookup-create llama-lookup-merge llama-lookup-stats llama-parallel llama-passkey llama-perplexity llama-q8dot llama-quantize llama-quantize-stats llama-retrieval llama-save-load-state llama-server llama-simple llama-speculative llama-tokenize llama-train-text-from-scratch llama-vdot llama-cvector-generator tests/test-c.o\n",
            "rm -rvf tests/test-autorelease tests/test-backend-ops tests/test-chat-template tests/test-double-float tests/test-grad0 tests/test-grammar-integration tests/test-grammar-parser tests/test-json-schema-to-grammar tests/test-llama-grammar tests/test-model-load-cancel tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-rope tests/test-sampling tests/test-tokenizer-0 tests/test-tokenizer-1-bpe tests/test-tokenizer-1-spm\n",
            "rm -rvf main quantize quantize-stats perplexity imatrix embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf gguf-split eval-callback llama-bench libllava.a llava-cli baby-llama retrieval speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead lookup passkey gritlm\n",
            "find examples pocs -type f -name \"*.o\" -delete\n",
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE \n",
            "I NVCCFLAGS: -std=c++11 -O3 \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c ggml/src/sgemm.cpp -o ggml/src/sgemm.o\n",
            "cc  -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion    -c ggml/src/ggml.c -o ggml/src/ggml.o\n",
            "cc  -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion    -c ggml/src/ggml-alloc.c -o ggml/src/ggml-alloc.o\n",
            "cc  -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion    -c ggml/src/ggml-backend.c -o ggml/src/ggml-backend.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion     -c ggml/src/ggml-quants.c -o ggml/src/ggml-quants.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c src/llama.cpp -o src/llama.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c src/unicode.cpp -o src/unicode.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c src/unicode-data.cpp -o src/unicode-data.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/common.cpp -o common/common.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/console.cpp -o common/console.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/ngram-cache.cpp -o common/ngram-cache.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/sampling.cpp -o common/sampling.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/train.cpp -o common/train.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/grammar-parser.cpp -o common/grammar-parser.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/json-schema-to-grammar.cpp -o common/json-schema-to-grammar.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -Iexamples/gguf-hash/deps -c examples/gguf-hash/deps/sha1/sha1.c -o examples/gguf-hash/deps/sha1/sha1.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -Iexamples/gguf-hash/deps -c examples/gguf-hash/deps/xxhash/xxhash.c -o examples/gguf-hash/deps/xxhash/xxhash.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -Iexamples/gguf-hash/deps -c examples/gguf-hash/deps/sha256/sha256.c -o examples/gguf-hash/deps/sha256/sha256.o\n",
            "cc -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c tests/test-double-float.cpp -o tests/test-double-float.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c common/build-info.cpp -o common/build-info.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE   tests/test-double-float.o -o tests/test-double-float  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/benchmark/benchmark-matmult.cpp -o examples/benchmark/benchmark-matmult.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/export-lora/export-lora.cpp -o examples/export-lora/export-lora.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/gguf/gguf.cpp -o examples/gguf/gguf.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c pocs/vdot/q8dot.cpp -o pocs/vdot/q8dot.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c pocs/vdot/vdot.cpp -o pocs/vdot/vdot.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c tests/test-backend-ops.cpp -o tests/test-backend-ops.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c tests/test-grad0.cpp -o tests/test-grad0.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c tests/test-opt.cpp -o tests/test-opt.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c tests/test-quantize-fns.cpp -o tests/test-quantize-fns.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c tests/test-quantize-perf.cpp -o tests/test-quantize-perf.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c tests/test-rope.cpp -o tests/test-rope.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o tests/test-opt.o -o tests/test-opt  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/ggml.o ggml/src/sgemm.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o tests/test-rope.o -o tests/test-rope  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o tests/test-quantize-fns.o -o tests/test-quantize-fns  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o examples/gguf/gguf.o -o llama-gguf  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/ggml.o ggml/src/sgemm.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o pocs/vdot/vdot.o -o llama-vdot  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o common/build-info.o examples/benchmark/benchmark-matmult.o -o llama-benchmark-matmult  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/ggml.o ggml/src/sgemm.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o pocs/vdot/q8dot.o -o llama-q8dot  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o tests/test-grad0.o -o tests/test-grad0  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o tests/test-quantize-perf.o -o tests/test-quantize-perf  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o examples/export-lora/export-lora.o -o llama-export-lora  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o tests/test-backend-ops.o -o tests/test-backend-ops  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c tests/test-llama-grammar.cpp -o tests/test-llama-grammar.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/baby-llama/baby-llama.cpp -o examples/baby-llama/baby-llama.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/batched/batched.cpp -o examples/batched/batched.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/batched-bench/batched-bench.cpp -o examples/batched-bench/batched-bench.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/llama-bench/llama-bench.cpp -o examples/llama-bench/llama-bench.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/main/main.cpp -o examples/main/main.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp -o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/embedding/embedding.cpp -o examples/embedding/embedding.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/eval-callback/eval-callback.cpp -o examples/eval-callback/eval-callback.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/finetune/finetune.cpp -o examples/finetune/finetune.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/gbnf-validator/gbnf-validator.cpp -o examples/gbnf-validator/gbnf-validator.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -Iexamples/gguf-hash/deps -c examples/gguf-hash/gguf-hash.cpp -o examples/gguf-hash/gguf-hash.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/gguf-split/gguf-split.cpp -o examples/gguf-split/gguf-split.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/gritlm/gritlm.cpp -o examples/gritlm/gritlm.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/imatrix/imatrix.cpp -o examples/imatrix/imatrix.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/infill/infill.cpp -o examples/infill/infill.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/llava/llava-cli.cpp -o examples/llava/llava-cli.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/lookahead/lookahead.cpp -o examples/lookahead/lookahead.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup.cpp -o examples/lookup/lookup.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-create.cpp -o examples/lookup/lookup-create.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-merge.cpp -o examples/lookup/lookup-merge.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/lookup/lookup-stats.cpp -o examples/lookup/lookup-stats.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/parallel/parallel.cpp -o examples/parallel/parallel.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/passkey/passkey.cpp -o examples/passkey/passkey.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/perplexity/perplexity.cpp -o examples/perplexity/perplexity.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/quantize/quantize.cpp -o examples/quantize/quantize.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/quantize-stats/quantize-stats.cpp -o examples/quantize-stats/quantize-stats.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/retrieval/retrieval.cpp -o examples/retrieval/retrieval.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/save-load-state/save-load-state.cpp -o examples/save-load-state/save-load-state.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/server/server.cpp -o examples/server/server.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/simple/simple.cpp -o examples/simple/simple.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/speculative/speculative.cpp -o examples/speculative/speculative.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/tokenize/tokenize.cpp -o examples/tokenize/tokenize.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/train-text-from-scratch/train-text-from-scratch.cpp -o examples/train-text-from-scratch/train-text-from-scratch.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/cvector-generator/cvector-generator.cpp -o examples/cvector-generator/cvector-generator.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c tests/test-autorelease.cpp -o tests/test-autorelease.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c tests/test-chat-template.cpp -o tests/test-chat-template.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c tests/test-grammar-integration.cpp -o tests/test-grammar-integration.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c tests/test-grammar-parser.cpp -o tests/test-grammar-parser.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -Iexamples/server -c tests/test-json-schema-to-grammar.cpp -o tests/test-json-schema-to-grammar.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c tests/test-model-load-cancel.cpp -o tests/test-model-load-cancel.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c tests/test-sampling.cpp -o tests/test-sampling.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c tests/test-tokenizer-0.cpp -o tests/test-tokenizer-0.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c tests/test-tokenizer-1-bpe.cpp -o tests/test-tokenizer-1-bpe.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c tests/test-tokenizer-1-spm.cpp -o tests/test-tokenizer-1-spm.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  tests/get-model.cpp ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o tests/test-model-load-cancel.o -o tests/test-model-load-cancel  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  tests/get-model.cpp ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o tests/test-autorelease.o -o tests/test-autorelease  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/baby-llama/baby-llama.o -o llama-baby-llama  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/tokenize/tokenize.o -o llama-tokenize  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/lookup/lookup-merge.o -o llama-lookup-merge  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o tests/test-tokenizer-1-bpe.o -o tests/test-tokenizer-1-bpe  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  examples/gguf-hash/deps/sha1/sha1.o examples/gguf-hash/deps/xxhash/xxhash.o examples/gguf-hash/deps/sha256/sha256.o ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/gguf-hash/gguf-hash.o -o llama-gguf-hash  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/gguf-split/gguf-split.o -o llama-gguf-split  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/gbnf-validator/gbnf-validator.o -o llama-gbnf-validator  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o tests/test-sampling.o -o tests/test-sampling  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o tests/test-tokenizer-1-spm.o -o tests/test-tokenizer-1-spm  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o tests/test-chat-template.o -o tests/test-chat-template  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o tests/test-grammar-parser.o -o tests/test-grammar-parser  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/eval-callback/eval-callback.o -o llama-eval-callback  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/lookup/lookup-create.o -o llama-lookup-create  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/embedding/embedding.o -o llama-embedding  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/gritlm/gritlm.o -o llama-gritlm  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/save-load-state/save-load-state.o -o llama-save-load-state  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/batched-bench/batched-bench.o -o llama-batched-bench  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/simple/simple.o -o llama-simple  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/passkey/passkey.o -o llama-passkey  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/lookup/lookup-stats.o -o llama-lookup-stats  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/batched/batched.o -o llama-batched  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o tests/test-tokenizer-0.o -o tests/test-tokenizer-0  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/train-text-from-scratch/train-text-from-scratch.o -o llama-train-text-from-scratch  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/lookup/lookup.o -o llama-lookup  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/retrieval/retrieval.o -o llama-retrieval  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/llava/clip.cpp  -o examples/llava/clip.o -Wno-cast-qual\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/quantize/quantize.o -o llama-quantize  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/lookahead/lookahead.o -o llama-lookahead  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/parallel/parallel.o -o llama-parallel  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o -o llama-convert-llama2c-to-ggml  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/cvector-generator/cvector-generator.o -o llama-cvector-generator  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/speculative/speculative.o -o llama-speculative  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/finetune/finetune.o -o llama-finetune  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/infill/infill.o -o llama-infill  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/imatrix/imatrix.o -o llama-imatrix  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/main/main.o -o llama-cli  \n",
            "\n",
            "====  Run ./llama-cli -h for help.  ====\n",
            "\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/quantize-stats/quantize-stats.o -o llama-quantize-stats  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/perplexity/perplexity.o -o llama-perplexity  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o tests/test-grammar-integration.o -o tests/test-grammar-integration  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/llama-bench/llama-bench.o -o llama-bench  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o tests/test-json-schema-to-grammar.o -o tests/test-json-schema-to-grammar  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/llava/llava.cpp -o examples/llava/llava.o\n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o src/unicode.o src/unicode-data.o tests/test-llama-grammar.o -o tests/test-llama-grammar  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o examples/llava/llava-cli.o examples/llava/clip.o examples/llava/llava.o -o llama-llava-cli  \n",
            "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  ggml/src/sgemm.o ggml/src/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o src/llama.o src/unicode.o src/unicode-data.o common/common.o common/console.o common/ngram-cache.o common/sampling.o common/train.o common/grammar-parser.o common/build-info.o common/json-schema-to-grammar.o -Iexamples/server examples/server/server.o -o llama-server   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model.save_pretrained_gguf(outname, tokenizer, quantization_method = [ \"f16\", \"q4_k_m\", \"q6_k\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ4C_IfgXraP",
        "outputId": "2bb625bf-70bb-41b8-8ff3-d3df9498af24"
      },
      "id": "yJ4C_IfgXraP",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 63.27 out of 83.48 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:55<00:00,  1.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Done.\n",
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['f16', 'q4_k_m', 'q6_k'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: [1] Converting model at phi-4-r16_a16_s0_d0_bnone_l0.0001_4bFalse_20250514_2224-cp-4745 into f16 GGUF format.\n",
            "The output location will be /content/phi-4-r16_a16_s0_d0_bnone_l0.0001_4bFalse_20250514_2224-cp-4745/unsloth.F16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: phi-4-r16_a16_s0_d0_bnone_l0.0001_4bFalse_20250514_2224-cp-4745\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 16384\n",
            "INFO:hf-to-gguf:gguf: embedding length = 5120\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 17920\n",
            "INFO:hf-to-gguf:gguf: head count = 40\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 10\n",
            "INFO:hf-to-gguf:gguf: rope theta = 250000\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 12 threads.\n",
            "INFO:gguf.vocab:Adding 100000 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 100257\n",
            "INFO:gguf.vocab:Setting special token type eos to 100265\n",
            "INFO:gguf.vocab:Setting special token type unk to 5809\n",
            "INFO:gguf.vocab:Setting special token type pad to 100351\n",
            "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if (message['role'] == 'system') %}{{'<|im_start|>system<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'user') %}{{'<|im_start|>user<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'assistant') %}{{'<|im_start|>assistant<|im_sep|>' + message['content'] + '<|im_end|>'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant<|im_sep|>' }}{% endif %}\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> F16, shape = {5120, 100352}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00005-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.32.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.32.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.32.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.32.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.32.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.32.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.33.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.33.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.33.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.33.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.33.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.33.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.33.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.33.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.33.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.34.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.34.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.34.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.34.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00006-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> F16, shape = {5120, 100352}\n",
            "INFO:hf-to-gguf:blk.34.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.34.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.34.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.35.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.35.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.35.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.35.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.35.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.36.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.36.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.36.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.36.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.36.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.36.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.37.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.37.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.37.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.37.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.37.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.37.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.38.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.38.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.38.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.38.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.38.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.38.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.39.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.39.ffn_down.weight,      torch.bfloat16 --> F16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.39.ffn_gate.weight,      torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.39.ffn_up.weight,        torch.bfloat16 --> F16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.39.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_k.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.39.attn_output.weight,   torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_q.weight,        torch.bfloat16 --> F16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_v.weight,        torch.bfloat16 --> F16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/phi-4-r16_a16_s0_d0_bnone_l0.0001_4bFalse_20250514_2224-cp-4745/unsloth.F16.gguf: n_tensors = 363, total_size = 29.3G\n",
            "Writing: 100%|██████████| 29.3G/29.3G [01:36<00:00, 304Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/phi-4-r16_a16_s0_d0_bnone_l0.0001_4bFalse_20250514_2224-cp-4745/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/phi-4-r16_a16_s0_d0_bnone_l0.0001_4bFalse_20250514_2224-cp-4745/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 1 (2ee44c9)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/phi-4-r16_a16_s0_d0_bnone_l0.0001_4bFalse_20250514_2224-cp-4745/unsloth.F16.gguf' to '/content/phi-4-r16_a16_s0_d0_bnone_l0.0001_4bFalse_20250514_2224-cp-4745/unsloth.Q4_K_M.gguf' as Q4_K_M using 24 threads\n",
            "llama_model_loader: loaded meta data with 25 key-value pairs and 363 tensors from /content/phi-4-r16_a16_s0_d0_bnone_l0.0001_4bFalse_20250514_2224-cp-4745/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = phi-4-r16_a16_s0_d0_bnone_l0.0001_4bF...\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 16384\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 10\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 250000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 100352\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = dbrx\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,100352]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,100352]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,100000]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 100257\n",
            "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 100265\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.unknown_token_id u32              = 5809\n",
            "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 100351\n",
            "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% for message in messages %}{% if (m...\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type  f16:  282 tensors\n",
            "[   1/ 363]                    token_embd.weight - [ 5120, 100352,     1,     1], type =    f16, converting to q4_K .. size =   980.00 MiB ->   275.62 MiB\n",
            "[   2/ 363]               blk.0.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   3/ 363]                blk.0.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[   4/ 363]                blk.0.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[   5/ 363]                  blk.0.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[   6/ 363]                blk.0.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   7/ 363]                  blk.0.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[   8/ 363]             blk.0.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[   9/ 363]                  blk.0.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  10/ 363]                  blk.0.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  11/ 363]               blk.1.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  12/ 363]                blk.1.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  13/ 363]                blk.1.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  14/ 363]                  blk.1.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  15/ 363]                blk.1.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  16/ 363]                  blk.1.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  17/ 363]             blk.1.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  18/ 363]                  blk.1.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  19/ 363]                  blk.1.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  20/ 363]               blk.2.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  21/ 363]                blk.2.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  22/ 363]                blk.2.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  23/ 363]                  blk.2.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  24/ 363]                blk.2.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  25/ 363]                  blk.2.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  26/ 363]             blk.2.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  27/ 363]                  blk.2.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  28/ 363]                  blk.2.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  29/ 363]               blk.3.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  30/ 363]                blk.3.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  31/ 363]                blk.3.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  32/ 363]                  blk.3.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  33/ 363]                blk.3.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  34/ 363]                  blk.3.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  35/ 363]             blk.3.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  36/ 363]                  blk.3.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  37/ 363]                  blk.3.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  38/ 363]               blk.4.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  39/ 363]                blk.4.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  40/ 363]                blk.4.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  41/ 363]                  blk.4.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  42/ 363]                blk.4.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  43/ 363]                  blk.4.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  44/ 363]             blk.4.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  45/ 363]                  blk.4.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  46/ 363]                  blk.4.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  47/ 363]                blk.5.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  48/ 363]                  blk.5.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  49/ 363]                  blk.5.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  50/ 363]             blk.5.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  51/ 363]                  blk.5.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  52/ 363]                  blk.5.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  53/ 363]              blk.10.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  54/ 363]               blk.10.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  55/ 363]               blk.10.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  56/ 363]                 blk.10.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  57/ 363]               blk.10.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  58/ 363]                 blk.10.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  59/ 363]            blk.10.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  60/ 363]                 blk.10.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  61/ 363]                 blk.10.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  62/ 363]              blk.11.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  63/ 363]               blk.11.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  64/ 363]               blk.11.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  65/ 363]                 blk.11.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  66/ 363]               blk.11.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  67/ 363]                 blk.11.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  68/ 363]            blk.11.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  69/ 363]                 blk.11.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  70/ 363]                 blk.11.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  71/ 363]              blk.12.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  72/ 363]               blk.12.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  73/ 363]               blk.12.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  74/ 363]                 blk.12.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  75/ 363]               blk.12.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  76/ 363]                 blk.12.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  77/ 363]            blk.12.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  78/ 363]                 blk.12.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  79/ 363]                 blk.12.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  80/ 363]               blk.5.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  81/ 363]                blk.5.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  82/ 363]                blk.5.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  83/ 363]               blk.6.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  84/ 363]                blk.6.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  85/ 363]                blk.6.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  86/ 363]                  blk.6.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  87/ 363]                blk.6.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  88/ 363]                  blk.6.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  89/ 363]             blk.6.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  90/ 363]                  blk.6.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  91/ 363]                  blk.6.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  92/ 363]               blk.7.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  93/ 363]                blk.7.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  94/ 363]                blk.7.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  95/ 363]                  blk.7.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  96/ 363]                blk.7.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  97/ 363]                  blk.7.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  98/ 363]             blk.7.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  99/ 363]                  blk.7.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 100/ 363]                  blk.7.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 101/ 363]               blk.8.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 102/ 363]                blk.8.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 103/ 363]                blk.8.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 104/ 363]                  blk.8.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 105/ 363]                blk.8.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 106/ 363]                  blk.8.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 107/ 363]             blk.8.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 108/ 363]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 109/ 363]                  blk.8.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 110/ 363]               blk.9.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 111/ 363]                blk.9.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 112/ 363]                blk.9.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 113/ 363]                  blk.9.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 114/ 363]                blk.9.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 115/ 363]                  blk.9.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 116/ 363]             blk.9.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 117/ 363]                  blk.9.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 118/ 363]                  blk.9.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 119/ 363]              blk.13.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 120/ 363]               blk.13.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 121/ 363]               blk.13.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 122/ 363]                 blk.13.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 123/ 363]               blk.13.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 124/ 363]                 blk.13.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 125/ 363]            blk.13.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 126/ 363]                 blk.13.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 127/ 363]                 blk.13.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 128/ 363]              blk.14.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 129/ 363]               blk.14.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 130/ 363]               blk.14.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 131/ 363]                 blk.14.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 132/ 363]               blk.14.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 133/ 363]                 blk.14.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 134/ 363]            blk.14.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 135/ 363]                 blk.14.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 136/ 363]                 blk.14.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 137/ 363]              blk.15.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 138/ 363]               blk.15.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 139/ 363]               blk.15.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 140/ 363]                 blk.15.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 141/ 363]               blk.15.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 142/ 363]                 blk.15.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 143/ 363]            blk.15.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 144/ 363]                 blk.15.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 145/ 363]                 blk.15.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 146/ 363]              blk.16.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 147/ 363]               blk.16.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 148/ 363]               blk.16.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 149/ 363]                 blk.16.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 150/ 363]               blk.16.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 151/ 363]                 blk.16.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 152/ 363]            blk.16.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 153/ 363]                 blk.16.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 154/ 363]                 blk.16.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 155/ 363]              blk.17.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 156/ 363]               blk.17.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 157/ 363]               blk.17.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 158/ 363]                 blk.17.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 159/ 363]               blk.17.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 160/ 363]                 blk.17.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 161/ 363]            blk.17.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 162/ 363]                 blk.17.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 163/ 363]                 blk.17.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 164/ 363]              blk.18.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 165/ 363]               blk.18.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 166/ 363]               blk.18.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 167/ 363]                 blk.18.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 168/ 363]               blk.18.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 169/ 363]                 blk.18.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 170/ 363]            blk.18.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 171/ 363]                 blk.18.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 172/ 363]                 blk.18.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 173/ 363]              blk.19.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 174/ 363]               blk.19.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 175/ 363]               blk.19.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 176/ 363]                 blk.19.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 177/ 363]               blk.19.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 178/ 363]                 blk.19.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 179/ 363]            blk.19.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 180/ 363]                 blk.19.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 181/ 363]                 blk.19.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 182/ 363]                 blk.20.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 183/ 363]            blk.20.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 184/ 363]                 blk.20.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 185/ 363]                 blk.20.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 186/ 363]              blk.20.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 187/ 363]               blk.20.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 188/ 363]               blk.20.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 189/ 363]                 blk.20.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 190/ 363]               blk.20.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 191/ 363]              blk.21.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 192/ 363]               blk.21.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 193/ 363]               blk.21.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 194/ 363]                 blk.21.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 195/ 363]               blk.21.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 196/ 363]                 blk.21.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 197/ 363]            blk.21.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 198/ 363]                 blk.21.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 199/ 363]                 blk.21.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 200/ 363]              blk.22.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 201/ 363]               blk.22.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 202/ 363]               blk.22.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 203/ 363]                 blk.22.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 204/ 363]               blk.22.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 205/ 363]                 blk.22.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 206/ 363]            blk.22.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 207/ 363]                 blk.22.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 208/ 363]                 blk.22.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 209/ 363]              blk.23.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 210/ 363]               blk.23.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 211/ 363]               blk.23.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 212/ 363]                 blk.23.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 213/ 363]               blk.23.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 214/ 363]                 blk.23.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 215/ 363]            blk.23.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 216/ 363]                 blk.23.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 217/ 363]                 blk.23.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 218/ 363]              blk.24.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 219/ 363]               blk.24.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 220/ 363]               blk.24.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 221/ 363]                 blk.24.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 222/ 363]               blk.24.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 223/ 363]                 blk.24.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 224/ 363]            blk.24.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 225/ 363]                 blk.24.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 226/ 363]                 blk.24.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 227/ 363]              blk.25.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 228/ 363]               blk.25.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 229/ 363]               blk.25.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 230/ 363]                 blk.25.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 231/ 363]               blk.25.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 232/ 363]                 blk.25.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 233/ 363]            blk.25.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 234/ 363]                 blk.25.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 235/ 363]                 blk.25.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 236/ 363]              blk.26.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 237/ 363]               blk.26.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 238/ 363]               blk.26.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 239/ 363]                 blk.26.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 240/ 363]               blk.26.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 241/ 363]                 blk.26.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 242/ 363]            blk.26.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 243/ 363]                 blk.26.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 244/ 363]                 blk.26.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 245/ 363]               blk.27.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 246/ 363]                 blk.27.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 247/ 363]            blk.27.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 248/ 363]                 blk.27.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 249/ 363]                 blk.27.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 250/ 363]              blk.27.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 251/ 363]               blk.27.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 252/ 363]                 blk.27.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 253/ 363]               blk.27.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 254/ 363]              blk.28.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 255/ 363]               blk.28.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 256/ 363]               blk.28.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 257/ 363]                 blk.28.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 258/ 363]               blk.28.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 259/ 363]                 blk.28.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 260/ 363]            blk.28.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 261/ 363]                 blk.28.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 262/ 363]                 blk.28.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 263/ 363]              blk.29.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 264/ 363]               blk.29.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 265/ 363]               blk.29.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 266/ 363]                 blk.29.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 267/ 363]               blk.29.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 268/ 363]                 blk.29.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 269/ 363]            blk.29.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 270/ 363]                 blk.29.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 271/ 363]                 blk.29.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 272/ 363]              blk.30.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 273/ 363]               blk.30.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 274/ 363]               blk.30.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 275/ 363]                 blk.30.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 276/ 363]               blk.30.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 277/ 363]                 blk.30.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 278/ 363]            blk.30.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 279/ 363]                 blk.30.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 280/ 363]                 blk.30.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 281/ 363]              blk.31.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 282/ 363]               blk.31.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 283/ 363]               blk.31.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 284/ 363]                 blk.31.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 285/ 363]               blk.31.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 286/ 363]                 blk.31.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 287/ 363]            blk.31.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 288/ 363]                 blk.31.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 289/ 363]                 blk.31.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 290/ 363]              blk.32.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 291/ 363]               blk.32.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 292/ 363]               blk.32.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 293/ 363]                 blk.32.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 294/ 363]               blk.32.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 295/ 363]                 blk.32.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 296/ 363]            blk.32.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 297/ 363]                 blk.32.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 298/ 363]                 blk.32.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 299/ 363]              blk.33.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 300/ 363]               blk.33.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 301/ 363]               blk.33.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 302/ 363]                 blk.33.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 303/ 363]               blk.33.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 304/ 363]                 blk.33.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 305/ 363]            blk.33.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 306/ 363]                 blk.33.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 307/ 363]                 blk.33.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 308/ 363]               blk.34.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 309/ 363]                 blk.34.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 310/ 363]                 blk.34.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 311/ 363]            blk.34.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 312/ 363]                 blk.34.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 313/ 363]                 blk.34.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 314/ 363]                        output.weight - [ 5120, 100352,     1,     1], type =    f16, converting to q6_K .. size =   980.00 MiB ->   401.95 MiB\n",
            "[ 315/ 363]              blk.34.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 316/ 363]               blk.34.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 317/ 363]               blk.34.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 318/ 363]              blk.35.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 319/ 363]               blk.35.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 320/ 363]               blk.35.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 321/ 363]                 blk.35.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 322/ 363]               blk.35.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 323/ 363]                 blk.35.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 324/ 363]            blk.35.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 325/ 363]                 blk.35.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 326/ 363]                 blk.35.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 327/ 363]              blk.36.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 328/ 363]               blk.36.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 329/ 363]               blk.36.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 330/ 363]                 blk.36.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 331/ 363]               blk.36.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 332/ 363]                 blk.36.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 333/ 363]            blk.36.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 334/ 363]                 blk.36.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 335/ 363]                 blk.36.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 336/ 363]              blk.37.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 337/ 363]               blk.37.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 338/ 363]               blk.37.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 339/ 363]                 blk.37.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 340/ 363]               blk.37.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 341/ 363]                 blk.37.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 342/ 363]            blk.37.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 343/ 363]                 blk.37.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 344/ 363]                 blk.37.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 345/ 363]              blk.38.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 346/ 363]               blk.38.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 347/ 363]               blk.38.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 348/ 363]                 blk.38.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 349/ 363]               blk.38.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 350/ 363]                 blk.38.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 351/ 363]            blk.38.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 352/ 363]                 blk.38.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 353/ 363]                 blk.38.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 354/ 363]              blk.39.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 355/ 363]               blk.39.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 356/ 363]               blk.39.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 357/ 363]                 blk.39.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 358/ 363]               blk.39.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 359/ 363]                 blk.39.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 360/ 363]            blk.39.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 361/ 363]                 blk.39.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 362/ 363]                 blk.39.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 363/ 363]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "llama_model_quantize_internal: model size  = 27961.58 MB\n",
            "llama_model_quantize_internal: quant size  =  8475.06 MB\n",
            "\n",
            "main: quantize time = 263915.44 ms\n",
            "main:    total time = 263915.44 ms\n",
            "Unsloth: Conversion completed! Output location: /content/phi-4-r16_a16_s0_d0_bnone_l0.0001_4bFalse_20250514_2224-cp-4745/unsloth.Q4_K_M.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q6_k. This might take 20 minutes...\n",
            "main: build = 1 (2ee44c9)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/phi-4-r16_a16_s0_d0_bnone_l0.0001_4bFalse_20250514_2224-cp-4745/unsloth.F16.gguf' to '/content/phi-4-r16_a16_s0_d0_bnone_l0.0001_4bFalse_20250514_2224-cp-4745/unsloth.Q6_K.gguf' as Q6_K using 24 threads\n",
            "llama_model_loader: loaded meta data with 25 key-value pairs and 363 tensors from /content/phi-4-r16_a16_s0_d0_bnone_l0.0001_4bFalse_20250514_2224-cp-4745/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = phi-4-r16_a16_s0_d0_bnone_l0.0001_4bF...\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 16384\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 17920\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 10\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 250000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 100352\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = dbrx\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,100352]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,100352]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,100000]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 100257\n",
            "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 100265\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.unknown_token_id u32              = 5809\n",
            "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 100351\n",
            "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {% for message in messages %}{% if (m...\n",
            "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type  f16:  282 tensors\n",
            "[   1/ 363]                    token_embd.weight - [ 5120, 100352,     1,     1], type =    f16, converting to q6_K .. size =   980.00 MiB ->   401.95 MiB\n",
            "[   2/ 363]               blk.0.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   3/ 363]                blk.0.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[   4/ 363]                blk.0.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[   5/ 363]                  blk.0.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[   6/ 363]                blk.0.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   7/ 363]                  blk.0.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[   8/ 363]             blk.0.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[   9/ 363]                  blk.0.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  10/ 363]                  blk.0.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  11/ 363]               blk.1.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  12/ 363]                blk.1.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  13/ 363]                blk.1.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  14/ 363]                  blk.1.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  15/ 363]                blk.1.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  16/ 363]                  blk.1.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  17/ 363]             blk.1.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  18/ 363]                  blk.1.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  19/ 363]                  blk.1.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  20/ 363]               blk.2.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  21/ 363]                blk.2.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  22/ 363]                blk.2.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  23/ 363]                  blk.2.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  24/ 363]                blk.2.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  25/ 363]                  blk.2.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  26/ 363]             blk.2.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  27/ 363]                  blk.2.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  28/ 363]                  blk.2.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  29/ 363]               blk.3.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  30/ 363]                blk.3.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  31/ 363]                blk.3.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  32/ 363]                  blk.3.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  33/ 363]                blk.3.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  34/ 363]                  blk.3.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  35/ 363]             blk.3.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  36/ 363]                  blk.3.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  37/ 363]                  blk.3.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  38/ 363]               blk.4.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  39/ 363]                blk.4.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  40/ 363]                blk.4.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  41/ 363]                  blk.4.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  42/ 363]                blk.4.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  43/ 363]                  blk.4.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  44/ 363]             blk.4.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  45/ 363]                  blk.4.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  46/ 363]                  blk.4.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  47/ 363]                blk.5.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  48/ 363]                  blk.5.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  49/ 363]                  blk.5.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  50/ 363]             blk.5.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  51/ 363]                  blk.5.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  52/ 363]                  blk.5.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  53/ 363]              blk.10.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  54/ 363]               blk.10.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  55/ 363]               blk.10.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  56/ 363]                 blk.10.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  57/ 363]               blk.10.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  58/ 363]                 blk.10.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  59/ 363]            blk.10.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  60/ 363]                 blk.10.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  61/ 363]                 blk.10.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  62/ 363]              blk.11.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  63/ 363]               blk.11.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  64/ 363]               blk.11.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  65/ 363]                 blk.11.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  66/ 363]               blk.11.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  67/ 363]                 blk.11.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  68/ 363]            blk.11.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  69/ 363]                 blk.11.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  70/ 363]                 blk.11.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  71/ 363]              blk.12.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  72/ 363]               blk.12.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  73/ 363]               blk.12.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  74/ 363]                 blk.12.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  75/ 363]               blk.12.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  76/ 363]                 blk.12.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  77/ 363]            blk.12.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  78/ 363]                 blk.12.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  79/ 363]                 blk.12.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  80/ 363]               blk.5.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  81/ 363]                blk.5.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  82/ 363]                blk.5.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  83/ 363]               blk.6.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  84/ 363]                blk.6.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  85/ 363]                blk.6.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  86/ 363]                  blk.6.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  87/ 363]                blk.6.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  88/ 363]                  blk.6.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  89/ 363]             blk.6.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  90/ 363]                  blk.6.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  91/ 363]                  blk.6.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  92/ 363]               blk.7.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  93/ 363]                blk.7.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  94/ 363]                blk.7.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  95/ 363]                  blk.7.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  96/ 363]                blk.7.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  97/ 363]                  blk.7.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  98/ 363]             blk.7.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[  99/ 363]                  blk.7.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 100/ 363]                  blk.7.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 101/ 363]               blk.8.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 102/ 363]                blk.8.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 103/ 363]                blk.8.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 104/ 363]                  blk.8.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 105/ 363]                blk.8.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 106/ 363]                  blk.8.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 107/ 363]             blk.8.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 108/ 363]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 109/ 363]                  blk.8.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 110/ 363]               blk.9.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 111/ 363]                blk.9.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 112/ 363]                blk.9.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 113/ 363]                  blk.9.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 114/ 363]                blk.9.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 115/ 363]                  blk.9.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 116/ 363]             blk.9.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 117/ 363]                  blk.9.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 118/ 363]                  blk.9.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 119/ 363]              blk.13.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 120/ 363]               blk.13.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 121/ 363]               blk.13.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 122/ 363]                 blk.13.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 123/ 363]               blk.13.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 124/ 363]                 blk.13.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 125/ 363]            blk.13.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 126/ 363]                 blk.13.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 127/ 363]                 blk.13.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 128/ 363]              blk.14.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 129/ 363]               blk.14.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 130/ 363]               blk.14.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 131/ 363]                 blk.14.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 132/ 363]               blk.14.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 133/ 363]                 blk.14.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 134/ 363]            blk.14.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 135/ 363]                 blk.14.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 136/ 363]                 blk.14.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 137/ 363]              blk.15.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 138/ 363]               blk.15.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 139/ 363]               blk.15.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 140/ 363]                 blk.15.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 141/ 363]               blk.15.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 142/ 363]                 blk.15.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 143/ 363]            blk.15.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 144/ 363]                 blk.15.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 145/ 363]                 blk.15.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 146/ 363]              blk.16.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 147/ 363]               blk.16.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 148/ 363]               blk.16.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 149/ 363]                 blk.16.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 150/ 363]               blk.16.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 151/ 363]                 blk.16.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 152/ 363]            blk.16.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 153/ 363]                 blk.16.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 154/ 363]                 blk.16.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 155/ 363]              blk.17.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 156/ 363]               blk.17.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 157/ 363]               blk.17.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 158/ 363]                 blk.17.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 159/ 363]               blk.17.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 160/ 363]                 blk.17.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 161/ 363]            blk.17.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 162/ 363]                 blk.17.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 163/ 363]                 blk.17.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 164/ 363]              blk.18.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 165/ 363]               blk.18.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 166/ 363]               blk.18.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 167/ 363]                 blk.18.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 168/ 363]               blk.18.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 169/ 363]                 blk.18.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 170/ 363]            blk.18.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 171/ 363]                 blk.18.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 172/ 363]                 blk.18.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 173/ 363]              blk.19.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 174/ 363]               blk.19.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 175/ 363]               blk.19.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 176/ 363]                 blk.19.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 177/ 363]               blk.19.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 178/ 363]                 blk.19.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 179/ 363]            blk.19.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 180/ 363]                 blk.19.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 181/ 363]                 blk.19.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 182/ 363]                 blk.20.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 183/ 363]            blk.20.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 184/ 363]                 blk.20.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 185/ 363]                 blk.20.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 186/ 363]              blk.20.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 187/ 363]               blk.20.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 188/ 363]               blk.20.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 189/ 363]                 blk.20.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 190/ 363]               blk.20.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 191/ 363]              blk.21.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 192/ 363]               blk.21.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 193/ 363]               blk.21.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 194/ 363]                 blk.21.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 195/ 363]               blk.21.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 196/ 363]                 blk.21.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 197/ 363]            blk.21.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 198/ 363]                 blk.21.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 199/ 363]                 blk.21.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 200/ 363]              blk.22.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 201/ 363]               blk.22.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 202/ 363]               blk.22.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 203/ 363]                 blk.22.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 204/ 363]               blk.22.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 205/ 363]                 blk.22.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 206/ 363]            blk.22.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 207/ 363]                 blk.22.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 208/ 363]                 blk.22.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 209/ 363]              blk.23.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 210/ 363]               blk.23.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 211/ 363]               blk.23.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 212/ 363]                 blk.23.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 213/ 363]               blk.23.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 214/ 363]                 blk.23.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 215/ 363]            blk.23.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 216/ 363]                 blk.23.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 217/ 363]                 blk.23.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 218/ 363]              blk.24.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 219/ 363]               blk.24.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 220/ 363]               blk.24.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 221/ 363]                 blk.24.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 222/ 363]               blk.24.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 223/ 363]                 blk.24.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 224/ 363]            blk.24.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 225/ 363]                 blk.24.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 226/ 363]                 blk.24.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 227/ 363]              blk.25.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 228/ 363]               blk.25.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 229/ 363]               blk.25.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 230/ 363]                 blk.25.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 231/ 363]               blk.25.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 232/ 363]                 blk.25.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 233/ 363]            blk.25.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 234/ 363]                 blk.25.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 235/ 363]                 blk.25.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 236/ 363]              blk.26.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 237/ 363]               blk.26.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 238/ 363]               blk.26.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 239/ 363]                 blk.26.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 240/ 363]               blk.26.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 241/ 363]                 blk.26.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 242/ 363]            blk.26.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 243/ 363]                 blk.26.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 244/ 363]                 blk.26.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 245/ 363]               blk.27.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 246/ 363]                 blk.27.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 247/ 363]            blk.27.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 248/ 363]                 blk.27.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 249/ 363]                 blk.27.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 250/ 363]              blk.27.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 251/ 363]               blk.27.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 252/ 363]                 blk.27.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 253/ 363]               blk.27.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 254/ 363]              blk.28.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 255/ 363]               blk.28.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 256/ 363]               blk.28.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 257/ 363]                 blk.28.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 258/ 363]               blk.28.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 259/ 363]                 blk.28.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 260/ 363]            blk.28.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 261/ 363]                 blk.28.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 262/ 363]                 blk.28.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 263/ 363]              blk.29.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 264/ 363]               blk.29.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 265/ 363]               blk.29.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 266/ 363]                 blk.29.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 267/ 363]               blk.29.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 268/ 363]                 blk.29.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 269/ 363]            blk.29.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 270/ 363]                 blk.29.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 271/ 363]                 blk.29.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 272/ 363]              blk.30.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 273/ 363]               blk.30.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 274/ 363]               blk.30.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 275/ 363]                 blk.30.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 276/ 363]               blk.30.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 277/ 363]                 blk.30.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 278/ 363]            blk.30.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 279/ 363]                 blk.30.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 280/ 363]                 blk.30.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 281/ 363]              blk.31.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 282/ 363]               blk.31.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 283/ 363]               blk.31.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 284/ 363]                 blk.31.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 285/ 363]               blk.31.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 286/ 363]                 blk.31.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 287/ 363]            blk.31.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 288/ 363]                 blk.31.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 289/ 363]                 blk.31.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 290/ 363]              blk.32.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 291/ 363]               blk.32.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 292/ 363]               blk.32.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 293/ 363]                 blk.32.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 294/ 363]               blk.32.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 295/ 363]                 blk.32.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 296/ 363]            blk.32.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 297/ 363]                 blk.32.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 298/ 363]                 blk.32.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 299/ 363]              blk.33.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 300/ 363]               blk.33.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 301/ 363]               blk.33.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 302/ 363]                 blk.33.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 303/ 363]               blk.33.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 304/ 363]                 blk.33.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 305/ 363]            blk.33.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 306/ 363]                 blk.33.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 307/ 363]                 blk.33.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 308/ 363]               blk.34.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 309/ 363]                 blk.34.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 310/ 363]                 blk.34.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 311/ 363]            blk.34.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 312/ 363]                 blk.34.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 313/ 363]                 blk.34.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 314/ 363]                        output.weight - [ 5120, 100352,     1,     1], type =    f16, converting to q6_K .. size =   980.00 MiB ->   401.95 MiB\n",
            "[ 315/ 363]              blk.34.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 316/ 363]               blk.34.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 317/ 363]               blk.34.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 318/ 363]              blk.35.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 319/ 363]               blk.35.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 320/ 363]               blk.35.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 321/ 363]                 blk.35.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 322/ 363]               blk.35.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 323/ 363]                 blk.35.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 324/ 363]            blk.35.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 325/ 363]                 blk.35.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 326/ 363]                 blk.35.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 327/ 363]              blk.36.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 328/ 363]               blk.36.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 329/ 363]               blk.36.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 330/ 363]                 blk.36.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 331/ 363]               blk.36.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 332/ 363]                 blk.36.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 333/ 363]            blk.36.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 334/ 363]                 blk.36.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 335/ 363]                 blk.36.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 336/ 363]              blk.37.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 337/ 363]               blk.37.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 338/ 363]               blk.37.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 339/ 363]                 blk.37.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 340/ 363]               blk.37.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 341/ 363]                 blk.37.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 342/ 363]            blk.37.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 343/ 363]                 blk.37.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 344/ 363]                 blk.37.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 345/ 363]              blk.38.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 346/ 363]               blk.38.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 347/ 363]               blk.38.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 348/ 363]                 blk.38.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 349/ 363]               blk.38.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 350/ 363]                 blk.38.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 351/ 363]            blk.38.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 352/ 363]                 blk.38.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 353/ 363]                 blk.38.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 354/ 363]              blk.39.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 355/ 363]               blk.39.ffn_down.weight - [17920,  5120,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 356/ 363]               blk.39.ffn_gate.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 357/ 363]                 blk.39.ffn_up.weight - [ 5120, 17920,     1,     1], type =    f16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 358/ 363]               blk.39.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 359/ 363]                 blk.39.attn_k.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 360/ 363]            blk.39.attn_output.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 361/ 363]                 blk.39.attn_q.weight - [ 5120,  5120,     1,     1], type =    f16, converting to q6_K .. size =    50.00 MiB ->    20.51 MiB\n",
            "[ 362/ 363]                 blk.39.attn_v.weight - [ 5120,  1280,     1,     1], type =    f16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 363/ 363]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "llama_model_quantize_internal: model size  = 27961.58 MB\n",
            "llama_model_quantize_internal: quant size  = 11469.55 MB\n",
            "\n",
            "main: quantize time = 97378.38 ms\n",
            "main:    total time = 97378.38 ms\n",
            "Unsloth: Conversion completed! Output location: /content/phi-4-r16_a16_s0_d0_bnone_l0.0001_4bFalse_20250514_2224-cp-4745/unsloth.Q6_K.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload data back"
      ],
      "metadata": {
        "id": "h18Ppwg6bab_"
      },
      "id": "h18Ppwg6bab_"
    },
    {
      "cell_type": "code",
      "source": [
        "rsyncto=\"result-pusher@kb.tfm.codigoparallevar.com:fine-tuning/fine-tuned/\" + trainset + \"/checkpoint-\" + checkpoint + \"/loadable\""
      ],
      "metadata": {
        "id": "baoxIvLPcnh-"
      },
      "id": "baoxIvLPcnh-",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Completed"
      ],
      "metadata": {
        "id": "H4xIePTwVUb5"
      },
      "id": "H4xIePTwVUb5"
    },
    {
      "cell_type": "code",
      "source": [
        "!rsync -HPrz --mkpath \"$outname\"/ \"$rsyncto\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7hpcFivbWpE",
        "outputId": "6dbad48c-ea4f-4420-cecf-4f853ab6220d"
      },
      "id": "p7hpcFivbWpE",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sending incremental file list\n",
            "config.json\n",
            "\r            793 100%    0.00kB/s    0:00:00  \r            793 100%    0.00kB/s    0:00:00 (xfr#1, to-chk=16/18)\n",
            "generation_config.json\n",
            "\r            170 100%  166.02kB/s    0:00:00  \r            170 100%  166.02kB/s    0:00:00 (xfr#2, to-chk=15/18)\n",
            "merges.txt\n",
            "\r         32,768   3%   31.25MB/s    0:00:00  \r        916,646 100%   79.47MB/s    0:00:00 (xfr#3, to-chk=14/18)\n",
            "model-00001-of-00006.safetensors\n",
            "  4,933,658,528 100%   19.67MB/s    0:03:59 (xfr#4, to-chk=13/18)\n",
            "model-00002-of-00006.safetensors\n",
            "  4,954,693,112 100%   19.60MB/s    0:04:01 (xfr#5, to-chk=12/18)\n",
            "model-00003-of-00006.safetensors\n",
            "  4,902,243,992 100%   19.69MB/s    0:03:57 (xfr#6, to-chk=11/18)\n",
            "model-00004-of-00006.safetensors\n",
            "  4,954,672,440 100%   19.67MB/s    0:04:00 (xfr#7, to-chk=10/18)\n",
            "model-00005-of-00006.safetensors\n",
            "  4,954,672,432 100%   19.68MB/s    0:04:00 (xfr#8, to-chk=9/18)\n",
            "model-00006-of-00006.safetensors\n",
            "  4,619,116,224 100%   19.63MB/s    0:03:44 (xfr#9, to-chk=8/18)\n",
            "model.safetensors.index.json\n",
            "         29,894 100%   37.24kB/s    0:00:00 (xfr#10, to-chk=7/18)\n",
            "special_tokens_map.json\n",
            "            570 100%    0.71kB/s    0:00:00 (xfr#11, to-chk=6/18)\n",
            "tokenizer.json\n",
            "      7,153,264 100%    7.35MB/s    0:00:00 (xfr#12, to-chk=5/18)\n",
            "tokenizer_config.json\n",
            "         17,989 100%   18.93kB/s    0:00:00 (xfr#13, to-chk=4/18)\n",
            "unsloth.F16.gguf\n",
            " 29,323,406,080 100%   20.03MB/s    0:23:16 (xfr#14, to-chk=3/18)\n",
            "unsloth.Q4_K_M.gguf\n",
            "  8,890,305,280 100%   15.66MB/s    0:09:01 (xfr#15, to-chk=2/18)\n",
            "unsloth.Q6_K.gguf\n",
            " 12,030,257,920 100%   15.42MB/s    0:12:24 (xfr#16, to-chk=1/18)\n",
            "vocab.json\n",
            "      1,612,637 100%    1.67MB/s    0:00:00 (xfr#17, to-chk=0/18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!date"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYxnGdunVWCC",
        "outputId": "c7061ef6-8fa2-4095-db6e-bc3cdbf8eab8"
      },
      "id": "tYxnGdunVWCC",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May 16 03:17:42 PM UTC 2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "WEgUSCzEbprN"
      },
      "id": "WEgUSCzEbprN",
      "execution_count": 14,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "454729cacebc4a1eab3df834ac6c471d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f29ce180dfce4e3787bba8f0f3cb1743",
              "IPY_MODEL_dbcca8d4362d4eb0bc3f731996630b4a",
              "IPY_MODEL_b5ef222392c54c00a3aa96f130d42297"
            ],
            "layout": "IPY_MODEL_6a4dcaa899c64574a18a086dad0aa15b"
          }
        },
        "f29ce180dfce4e3787bba8f0f3cb1743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_506a580e0da645e8a2ae7cf350eba80b",
            "placeholder": "​",
            "style": "IPY_MODEL_cd272ba4df6e400e9611bf04c4472727",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "dbcca8d4362d4eb0bc3f731996630b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cff2c842cc448ebbff4198c26ef36c9",
            "max": 29894,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_963fa7ca3faa40449273b686f6093234",
            "value": 29894
          }
        },
        "b5ef222392c54c00a3aa96f130d42297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a466cfa87ce64a919937e3e5d8e93bf0",
            "placeholder": "​",
            "style": "IPY_MODEL_fb98d399e1d945d6bc9e40c6b4200955",
            "value": " 29.9k/29.9k [00:00&lt;00:00, 3.32MB/s]"
          }
        },
        "6a4dcaa899c64574a18a086dad0aa15b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "506a580e0da645e8a2ae7cf350eba80b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd272ba4df6e400e9611bf04c4472727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cff2c842cc448ebbff4198c26ef36c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "963fa7ca3faa40449273b686f6093234": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a466cfa87ce64a919937e3e5d8e93bf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb98d399e1d945d6bc9e40c6b4200955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f516bac9163144fdbaa4f494d19f70b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0417b03a471446638b182fd1d0869c5c",
              "IPY_MODEL_8b80d3c0dba24efca01eb8bf8a1f8303",
              "IPY_MODEL_9ec2f682e40a4f93a0ff011f0fe76266"
            ],
            "layout": "IPY_MODEL_0e305a0dcf5f462f91277137983aa13f"
          }
        },
        "0417b03a471446638b182fd1d0869c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4d0701f3ba549aca169f5b126254fde",
            "placeholder": "​",
            "style": "IPY_MODEL_8287adc7ff964bb1a0b563e4aa4ace41",
            "value": "model-00001-of-00006.safetensors: 100%"
          }
        },
        "8b80d3c0dba24efca01eb8bf8a1f8303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32b1f9aae33c4a4c8e53e25b66999391",
            "max": 4933658528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2546fad4e90746edb0e7310382c7cd9a",
            "value": 4933658058
          }
        },
        "9ec2f682e40a4f93a0ff011f0fe76266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83ed389e5d3f46d8bcf3167e80b0df6b",
            "placeholder": "​",
            "style": "IPY_MODEL_9842b56467194eedb82e5c5198c83dbd",
            "value": " 4.93G/4.93G [00:15&lt;00:00, 549MB/s]"
          }
        },
        "0e305a0dcf5f462f91277137983aa13f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4d0701f3ba549aca169f5b126254fde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8287adc7ff964bb1a0b563e4aa4ace41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32b1f9aae33c4a4c8e53e25b66999391": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2546fad4e90746edb0e7310382c7cd9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83ed389e5d3f46d8bcf3167e80b0df6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9842b56467194eedb82e5c5198c83dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13411f05008d4c918242dc86fc11ab22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3966ebb0bb6c4af3a4c646288c5ad6db",
              "IPY_MODEL_e467f4f285b2405a8a17095c677e69dc",
              "IPY_MODEL_893a173f520d49e8810533106e6b5ba2"
            ],
            "layout": "IPY_MODEL_da50b878c8aa4f0aaacb85ea1639877a"
          }
        },
        "3966ebb0bb6c4af3a4c646288c5ad6db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc341fb160dd4cc9bf7dc0dcac7cd62b",
            "placeholder": "​",
            "style": "IPY_MODEL_a2d174540733429f90c141fcd20b45db",
            "value": "model-00002-of-00006.safetensors: 100%"
          }
        },
        "e467f4f285b2405a8a17095c677e69dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6edef4f5d95b41469d7a839b30351957",
            "max": 4954693112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ad0f9448a14463cb852c76492bef0f9",
            "value": 4954692640
          }
        },
        "893a173f520d49e8810533106e6b5ba2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f85be3d35c4b46bf9f064564bb7b7b83",
            "placeholder": "​",
            "style": "IPY_MODEL_08ae7efe8a794d75aa8d3555cd734191",
            "value": " 4.95G/4.95G [00:14&lt;00:00, 570MB/s]"
          }
        },
        "da50b878c8aa4f0aaacb85ea1639877a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc341fb160dd4cc9bf7dc0dcac7cd62b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2d174540733429f90c141fcd20b45db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6edef4f5d95b41469d7a839b30351957": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ad0f9448a14463cb852c76492bef0f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f85be3d35c4b46bf9f064564bb7b7b83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08ae7efe8a794d75aa8d3555cd734191": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af005114bef944b5bc049bbfcb3d9157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e33bba6d10140d0a4e6a4c4953900e1",
              "IPY_MODEL_de958341322f471081fb7437beff50bd",
              "IPY_MODEL_f100cd00502a4cce93a1f84bbc9f3b68"
            ],
            "layout": "IPY_MODEL_3121821ce7194a24a73438204b0201a3"
          }
        },
        "3e33bba6d10140d0a4e6a4c4953900e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d39f534a13140b3b46ce980f92a5847",
            "placeholder": "​",
            "style": "IPY_MODEL_8a70f90a68874eec99718f734d920455",
            "value": "model-00003-of-00006.safetensors: 100%"
          }
        },
        "de958341322f471081fb7437beff50bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_944bfb6cf29f4df68788d498751f53de",
            "max": 4902243992,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5000214e5024d69ab35e56dda6fc53b",
            "value": 4902243525
          }
        },
        "f100cd00502a4cce93a1f84bbc9f3b68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d3f1e578ee242f0930488b8f3925c29",
            "placeholder": "​",
            "style": "IPY_MODEL_a40d61ddb5d7441f98827ea4c0a6552a",
            "value": " 4.90G/4.90G [00:14&lt;00:00, 313MB/s]"
          }
        },
        "3121821ce7194a24a73438204b0201a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d39f534a13140b3b46ce980f92a5847": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a70f90a68874eec99718f734d920455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "944bfb6cf29f4df68788d498751f53de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5000214e5024d69ab35e56dda6fc53b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d3f1e578ee242f0930488b8f3925c29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a40d61ddb5d7441f98827ea4c0a6552a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb64c3bf5bd14991b5a994fb5ae7c887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e138b85ebe747c196e1355a750a903c",
              "IPY_MODEL_e7471a7b5e644c2491552c000b2a4f85",
              "IPY_MODEL_be5170065cf7463bb3e4312a81d7b50b"
            ],
            "layout": "IPY_MODEL_b295a7fa3f534dcdb6c49202bf211b63"
          }
        },
        "4e138b85ebe747c196e1355a750a903c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_019ddfc48f7f4cf39cb099eab1c25617",
            "placeholder": "​",
            "style": "IPY_MODEL_437f57ee7c83463cad2f0b0e895090f8",
            "value": "model-00004-of-00006.safetensors: 100%"
          }
        },
        "e7471a7b5e644c2491552c000b2a4f85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12b923f5fe1f44a8892aa12ac4f913d6",
            "max": 4954672440,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51f3aa7abeda496296c7f957edfc078c",
            "value": 4954671968
          }
        },
        "be5170065cf7463bb3e4312a81d7b50b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e559cc5e920472281442a08d3e2cb22",
            "placeholder": "​",
            "style": "IPY_MODEL_abd26025413b4ac6ac2887d011186b70",
            "value": " 4.95G/4.95G [00:14&lt;00:00, 259MB/s]"
          }
        },
        "b295a7fa3f534dcdb6c49202bf211b63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "019ddfc48f7f4cf39cb099eab1c25617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "437f57ee7c83463cad2f0b0e895090f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12b923f5fe1f44a8892aa12ac4f913d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51f3aa7abeda496296c7f957edfc078c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e559cc5e920472281442a08d3e2cb22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abd26025413b4ac6ac2887d011186b70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22ba7c98218b4c4985f60f30f0467aeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ec48f9da76b4dec8acaefd12253ae14",
              "IPY_MODEL_1f054aa82301497499dcbb835722712a",
              "IPY_MODEL_a540162b1353497a9539599b06536daa"
            ],
            "layout": "IPY_MODEL_6a7dc926dee1426fa9bf1a7cf038aadf"
          }
        },
        "0ec48f9da76b4dec8acaefd12253ae14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6855aef917824d3c86a4e2a3c25bbbab",
            "placeholder": "​",
            "style": "IPY_MODEL_56415b8e01a847538429f73d9db09ba5",
            "value": "model-00005-of-00006.safetensors: 100%"
          }
        },
        "1f054aa82301497499dcbb835722712a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25e8ee6cfc17486186936a9e0755cc58",
            "max": 4954672432,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cffad697d3bc4a40b6799ee49d4f24f6",
            "value": 4954671960
          }
        },
        "a540162b1353497a9539599b06536daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3f2da1690594306abba446c43054cc1",
            "placeholder": "​",
            "style": "IPY_MODEL_9e6450c597ec479a8fdb352e4d6a801e",
            "value": " 4.95G/4.95G [00:12&lt;00:00, 866MB/s]"
          }
        },
        "6a7dc926dee1426fa9bf1a7cf038aadf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6855aef917824d3c86a4e2a3c25bbbab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56415b8e01a847538429f73d9db09ba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25e8ee6cfc17486186936a9e0755cc58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cffad697d3bc4a40b6799ee49d4f24f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3f2da1690594306abba446c43054cc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e6450c597ec479a8fdb352e4d6a801e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3325fc9f7d0d4d09b861b45fb5ecc52e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d00a7bc34fc44d596f61e3617d8902c",
              "IPY_MODEL_de1e9946bdb74737a789d8115cd298f1",
              "IPY_MODEL_c5db8d88f42d461dab6659f10a6d4d44"
            ],
            "layout": "IPY_MODEL_e8f57f9c5ae64ad6a1e6ac10ae998bfd"
          }
        },
        "0d00a7bc34fc44d596f61e3617d8902c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5eaf212cba7b4a178f1e5b9dab223e51",
            "placeholder": "​",
            "style": "IPY_MODEL_443882bed12946b9a2e6c3eb0ba16f4d",
            "value": "model-00006-of-00006.safetensors: 100%"
          }
        },
        "de1e9946bdb74737a789d8115cd298f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23852c63fcfa4764978110f849b26c7f",
            "max": 4619116224,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_881dc453920f4b6690d050b64d86a6dc",
            "value": 4619115784
          }
        },
        "c5db8d88f42d461dab6659f10a6d4d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65bfa219ef624f40a8183a202754cd5e",
            "placeholder": "​",
            "style": "IPY_MODEL_fd915b67e1e243ab8ba045b9d3672612",
            "value": " 4.62G/4.62G [00:11&lt;00:00, 581MB/s]"
          }
        },
        "e8f57f9c5ae64ad6a1e6ac10ae998bfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5eaf212cba7b4a178f1e5b9dab223e51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "443882bed12946b9a2e6c3eb0ba16f4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23852c63fcfa4764978110f849b26c7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "881dc453920f4b6690d050b64d86a6dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65bfa219ef624f40a8183a202754cd5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd915b67e1e243ab8ba045b9d3672612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db82e45803504f3e92264bef4ea2a6ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93460b02c2f5449fa02bc02db39d2eed",
              "IPY_MODEL_424c41cf46f14db5b9173f9fa5a8d065",
              "IPY_MODEL_a8b8cbaa2dca4d57bb2eb0a0afe74fdc"
            ],
            "layout": "IPY_MODEL_456a1c976dfe4e91abf3093ac4bf511a"
          }
        },
        "93460b02c2f5449fa02bc02db39d2eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59bde084d81145dba069ee0fcd45caff",
            "placeholder": "​",
            "style": "IPY_MODEL_b092c00f75bb4f84afd3db1a931cacdd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "424c41cf46f14db5b9173f9fa5a8d065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4421d7115c1c423f839c99b24162fec8",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b2b73e7d13649639e27c4a908aed993",
            "value": 6
          }
        },
        "a8b8cbaa2dca4d57bb2eb0a0afe74fdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84db028fc23f4dec84c291d2720232f8",
            "placeholder": "​",
            "style": "IPY_MODEL_fc82395c2d594ddfa0cfeb377480e93e",
            "value": " 6/6 [00:08&lt;00:00,  1.35s/it]"
          }
        },
        "456a1c976dfe4e91abf3093ac4bf511a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59bde084d81145dba069ee0fcd45caff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b092c00f75bb4f84afd3db1a931cacdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4421d7115c1c423f839c99b24162fec8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b2b73e7d13649639e27c4a908aed993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84db028fc23f4dec84c291d2720232f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc82395c2d594ddfa0cfeb377480e93e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2a3f27543894789abef2418ceadd72c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c2a6ab52a7941b28e0f9a9f127ce3e4",
              "IPY_MODEL_74fb43a3edce4ef3b494cb7f4782b547",
              "IPY_MODEL_1ea1abf3dfde4424bcbe6d025637868f"
            ],
            "layout": "IPY_MODEL_1c33e3b569f24106b84aee21b545b11e"
          }
        },
        "2c2a6ab52a7941b28e0f9a9f127ce3e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05af7d6a236a442880e4bcf0c2343f4f",
            "placeholder": "​",
            "style": "IPY_MODEL_395ec87e10fb43a29fa2a94826900997",
            "value": "generation_config.json: 100%"
          }
        },
        "74fb43a3edce4ef3b494cb7f4782b547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcca808b3d9a46b6b3b25d940fc093b9",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5da200f4d2f4d3eb810cfc19b0a4124",
            "value": 147
          }
        },
        "1ea1abf3dfde4424bcbe6d025637868f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f13cb5783f074090866add3b3c5e924d",
            "placeholder": "​",
            "style": "IPY_MODEL_e3b443aacc654b93a7ac76c92a240208",
            "value": " 147/147 [00:00&lt;00:00, 19.8kB/s]"
          }
        },
        "1c33e3b569f24106b84aee21b545b11e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05af7d6a236a442880e4bcf0c2343f4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "395ec87e10fb43a29fa2a94826900997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcca808b3d9a46b6b3b25d940fc093b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5da200f4d2f4d3eb810cfc19b0a4124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f13cb5783f074090866add3b3c5e924d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3b443aacc654b93a7ac76c92a240208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6fb2f4bc56644a4b302b8e878d6ea76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e77d80df41444ee9c2aa41b9a67c125",
              "IPY_MODEL_669114be8e434d5790a1e94fa4db42b2",
              "IPY_MODEL_e06a6237698f4840bbbd6d2ca14425a8"
            ],
            "layout": "IPY_MODEL_6c3451649bbb48aeaec05fea1a486fad"
          }
        },
        "1e77d80df41444ee9c2aa41b9a67c125": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b19232948e4415290d65e6a3f5061de",
            "placeholder": "​",
            "style": "IPY_MODEL_95f7e5c8603c4835939ac3278150b75c",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "669114be8e434d5790a1e94fa4db42b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d2d25bc90364d0dabfe990871b9992f",
            "max": 17989,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74f916ad3f41454aa417c6bbdc4d4949",
            "value": 17989
          }
        },
        "e06a6237698f4840bbbd6d2ca14425a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31dacffb94da477fa1d769e2f3486ca5",
            "placeholder": "​",
            "style": "IPY_MODEL_1d0972522746445981633d6f1f41346a",
            "value": " 18.0k/18.0k [00:00&lt;00:00, 2.22MB/s]"
          }
        },
        "6c3451649bbb48aeaec05fea1a486fad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b19232948e4415290d65e6a3f5061de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95f7e5c8603c4835939ac3278150b75c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d2d25bc90364d0dabfe990871b9992f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74f916ad3f41454aa417c6bbdc4d4949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31dacffb94da477fa1d769e2f3486ca5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d0972522746445981633d6f1f41346a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a36f0f0cda04472832ac47a06a41c57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0af851b729c04386a3c18d6011ab498d",
              "IPY_MODEL_6544f59a839f4e8e99037c960bc2ab08",
              "IPY_MODEL_eb1aed097cdf455597d6bf9552c4146b"
            ],
            "layout": "IPY_MODEL_0fb8fff3759e48ca97b0bc35a9d474a4"
          }
        },
        "0af851b729c04386a3c18d6011ab498d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec50996065ea429db82cdd84d1218862",
            "placeholder": "​",
            "style": "IPY_MODEL_0006424cf72c4f2abfb6ecf05ecbc812",
            "value": "vocab.json: 100%"
          }
        },
        "6544f59a839f4e8e99037c960bc2ab08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4913d5ffea04a8f942bf8a046ff94c3",
            "max": 1612637,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80d0b6522ce64db2a8d41bcaedc0ae83",
            "value": 1612637
          }
        },
        "eb1aed097cdf455597d6bf9552c4146b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73c19736c4194fb18f230ee649b840d7",
            "placeholder": "​",
            "style": "IPY_MODEL_3b2d5328c42a40c0b44ab99c0030465f",
            "value": " 1.61M/1.61M [00:00&lt;00:00, 30.6MB/s]"
          }
        },
        "0fb8fff3759e48ca97b0bc35a9d474a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec50996065ea429db82cdd84d1218862": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0006424cf72c4f2abfb6ecf05ecbc812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4913d5ffea04a8f942bf8a046ff94c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80d0b6522ce64db2a8d41bcaedc0ae83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73c19736c4194fb18f230ee649b840d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b2d5328c42a40c0b44ab99c0030465f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dc2833fbe554bbda81ed611e52d326a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dce86e3a461942389e5a93be5ab59989",
              "IPY_MODEL_145d32b5fc554ee7b0f130241914add2",
              "IPY_MODEL_0b4c9224ee1849008dabd303948fc333"
            ],
            "layout": "IPY_MODEL_1bda6f402e914589824eaf4d626188df"
          }
        },
        "dce86e3a461942389e5a93be5ab59989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ec55b14d2c14139981b8099df635939",
            "placeholder": "​",
            "style": "IPY_MODEL_eb508be9310a476087946536c4860b43",
            "value": "merges.txt: 100%"
          }
        },
        "145d32b5fc554ee7b0f130241914add2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06cf576551f543368f32496ecd971389",
            "max": 916646,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e06fea0936a443ea8f8d622dec0b43a",
            "value": 916646
          }
        },
        "0b4c9224ee1849008dabd303948fc333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89016455188c45929d192681f2225cd6",
            "placeholder": "​",
            "style": "IPY_MODEL_fe90be9a01444b36bef06e092f29d71d",
            "value": " 917k/917k [00:00&lt;00:00, 27.8MB/s]"
          }
        },
        "1bda6f402e914589824eaf4d626188df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ec55b14d2c14139981b8099df635939": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb508be9310a476087946536c4860b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06cf576551f543368f32496ecd971389": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e06fea0936a443ea8f8d622dec0b43a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89016455188c45929d192681f2225cd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe90be9a01444b36bef06e092f29d71d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55d82245998e4ae79c2038fd6ada8a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71667b5c8fb74452acf930de576d1831",
              "IPY_MODEL_9047e24253f148ac8efa801707be781a",
              "IPY_MODEL_1288bc80818b4317b389c19bde8c41db"
            ],
            "layout": "IPY_MODEL_c3aeb290af764f208362c5f2bfdcb523"
          }
        },
        "71667b5c8fb74452acf930de576d1831": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a804ccd0f074e71b2a040b0f1227645",
            "placeholder": "​",
            "style": "IPY_MODEL_65e3450435be4e94b330014b0e7b9eb1",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "9047e24253f148ac8efa801707be781a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b96aed19ffb43b0855f773b772c24e4",
            "max": 456,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6dddfeb5fa3f46c8befd97b761d60a09",
            "value": 456
          }
        },
        "1288bc80818b4317b389c19bde8c41db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c00179a78a9146d18b2d9d411d739133",
            "placeholder": "​",
            "style": "IPY_MODEL_69a84d9652dd41af9d9799c3f5f5e232",
            "value": " 456/456 [00:00&lt;00:00, 57.9kB/s]"
          }
        },
        "c3aeb290af764f208362c5f2bfdcb523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a804ccd0f074e71b2a040b0f1227645": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65e3450435be4e94b330014b0e7b9eb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b96aed19ffb43b0855f773b772c24e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dddfeb5fa3f46c8befd97b761d60a09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c00179a78a9146d18b2d9d411d739133": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69a84d9652dd41af9d9799c3f5f5e232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d66c4c00d674d49ad2bb4d2391e31c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28a7574fe5e2471e872eb0802a7e4dc9",
              "IPY_MODEL_96bc9c9d5cdd47edbc943ae23a6a5f94",
              "IPY_MODEL_184ff96c7d914278b9f2489d67eed602"
            ],
            "layout": "IPY_MODEL_bc12e3e580ad4e3fb2325f19ec1dfe50"
          }
        },
        "28a7574fe5e2471e872eb0802a7e4dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e82b3caec784dd0b9aa1d63b5e6f107",
            "placeholder": "​",
            "style": "IPY_MODEL_eb7e47672e5a4884bd96e899f4e5890b",
            "value": "tokenizer.json: 100%"
          }
        },
        "96bc9c9d5cdd47edbc943ae23a6a5f94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62ceaab9d7064ac8b7680c9f83928cfe",
            "max": 7153264,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11b2b3ef92c84812855fd67ea09820ad",
            "value": 7153264
          }
        },
        "184ff96c7d914278b9f2489d67eed602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b03bb1ad4ef403796c489233611324a",
            "placeholder": "​",
            "style": "IPY_MODEL_363d723333bd47f3a5b504a52736308c",
            "value": " 7.15M/7.15M [00:00&lt;00:00, 33.2MB/s]"
          }
        },
        "bc12e3e580ad4e3fb2325f19ec1dfe50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e82b3caec784dd0b9aa1d63b5e6f107": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb7e47672e5a4884bd96e899f4e5890b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62ceaab9d7064ac8b7680c9f83928cfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11b2b3ef92c84812855fd67ea09820ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b03bb1ad4ef403796c489233611324a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "363d723333bd47f3a5b504a52736308c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}